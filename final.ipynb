{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# 获取脚本文件的所在目录\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "# 将当前工作目录更改为脚本所在的目录\n",
    "os.chdir(script_dir)\n",
    "\n",
    "base_url='https://www.vesselfinder.com'\n",
    "progress_filename = 'progress.json'\n",
    "\n",
    "'''\n",
    "使用 csv.writer\n",
    "csv.writer 返回一个写入器对象，允许逐行写入数据。writerow 方法用于写入一行数据，writerows 方法用于写入多行数据。\n",
    "'''\n",
    "num_csv=1\n",
    "scrapying_page_num=0\n",
    "current_boat_index = 0\n",
    "\n",
    "with open('test_for_vessel_finder'+str(num_csv)+'.csv', mode='a', newline='') as vfinder:\n",
    "    writer = csv.writer(vfinder)\n",
    "    writer.writerow(['IMO','MMSI', 'Callsign', 'Vessel Name', 'Ship type', 'Flag', 'Length Overall (m)', 'Beam (m)'])\n",
    "\n",
    "\n",
    "# 定义保存和加载进度的函数\n",
    "def save_progress(current_url, page, flag_index, type_value, num_csv, scrapying_page_num, current_boat_index, filename='progress.json'):\n",
    "    progress_data = {\n",
    "        'current_url': current_url,\n",
    "        'page': page,\n",
    "        'flag_index': flag_index,\n",
    "        'type_value': type_value,\n",
    "        'num_csv': num_csv,\n",
    "        'scrapying_page_num': scrapying_page_num,\n",
    "        'current_boat_index': current_boat_index\n",
    "    }\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(progress_data, f, indent=4)\n",
    "\n",
    "\n",
    "def load_progress(filename='progress.json'):\n",
    "    if os.path.exists(filename):\n",
    "        # print(os.path.abspath(filename))\n",
    "        with open(filename, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "# 定义cookies和headers\n",
    "cookies = {\n",
    "    'ROUTEID': '.1',\n",
    "    '_ga': 'GA1.1.866887217.1720492499',\n",
    "    'usprivacy': '1N--',\n",
    "    '_pbjs_userid_consent_data': '6683316680106290',\n",
    "    'cto_bundle': 'ioGBMF9oMzlnU2g4SXZDU0xPekloM0VZNGRNZEZvUktBTThYZGV5enlUR1RUNyUyQnBEZlZHallEUG5JMGdoYVJDRnpna3lCN2hBaCUyRk9MJTJGYnlzanJxVTd4RHBIJTJGQ3phaXZoMlQ2ZFpEaHFUT2dhM3NoVyUyQlJHR2x2RlVmSHclMkJVZExlSWZVbzdNWmdwT2R6c2V0a1E0WlV1d09FRnclM0QlM0Q',\n",
    "    'cto_bidid': 'CZBKLF9NYVpiYzFZZ2w4emJkOWI1JTJCRVRWeUlUTm1FNldLd29kWnZYWDJ0czhRWktadUlGWjk3SnpiUnpKODh2cWRoSWdvJTJGWHZVOEMwOEF6SzhZNzJDY2ZRaXRkZnRkQlkyYng5dGZjYSUyRmJId2ZPWlZscDVJTXM4NEdoOXdHNFZlRmhleg',\n",
    "    'cto_dna_bundle': 'TtsDkV9oMzlnU2g4SXZDU0xPekloM0VZNGRPdGtpSXpaczRGWHJNNUNFakg5ZmJwOXY2dVo5ViUyRlR0YlEyMjdVd0Q4R3RsTEZCSWZ3Y2hDQ1Q0RzhpSzlvVGJBJTNEJTNE',\n",
    "    '_ga_0MB1EVE8B7': 'GS1.1.1720500094.2.0.1720500094.0.0.0',\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'accept': 'clean_clean_text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',\n",
    "    # 'cookie': 'ROUTEID=.1; _ga=GA1.1.866887217.1720492499; usprivacy=1N--; _pbjs_userid_consent_data=6683316680106290; cto_bundle=ioGBMF9oMzlnU2g4SXZDU0xPekloM0VZNGRNZEZvUktBTThYZGV5enlUR1RUNyUyQnBEZlZHallEUG5JMGdoYVJDRnpna3lCN2hBaCUyRk9MJTJGYnlzanJxVTd4RHBIJTJGQ3phaXZoMlQ2ZFpEaHFUT2dhM3NoVyUyQlJHR2x2RlVmSHclMkJVZExlSWZVbzdNWmdwT2R6c2V0a1E0WlV1d09FRnclM0QlM0Q; cto_bidid=CZBKLF9NYVpiYzFZZ2w4emJkOWI1JTJCRVRWeUlUTm1FNldLd29kWnZYWDJ0czhRWktadUlGWjk3SnpiUnpKODh2cWRoSWdvJTJGWHZVOEMwOEF6SzhZNzJDY2ZRaXRkZnRkQlkyYng5dGZjYSUyRmJId2ZPWlZscDVJTXM4NEdoOXdHNFZlRmhleg; cto_dna_bundle=TtsDkV9oMzlnU2g4SXZDU0xPekloM0VZNGRPdGtpSXpaczRGWHJNNUNFakg5ZmJwOXY2dVo5ViUyRlR0YlEyMjdVd0Q4R3RsTEZCSWZ3Y2hDQ1Q0RzhpSzlvVGJBJTNEJTNE; _ga_0MB1EVE8B7=GS1.1.1720500094.2.0.1720500094.0.0.0',\n",
    "    'priority': 'u=0, i',\n",
    "    'referer': 'https://www.vesselfinder.com/vessels?page=2',\n",
    "    'sec-ch-ua': '\"Not/A)Brand\";v=\"8\", \"Chromium\";v=\"126\", \"Microsoft Edge\";v=\"126\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Windows\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boat_info_analysis(url):\n",
    "    global page, flag_index, type_value,num_csv,scrapying_page_num,current_boat_index\n",
    "    try:\n",
    "        time.sleep(random.uniform(1, 3))  # 随机延迟\n",
    "        response = requests.get(url, cookies=cookies, headers=headers)#cost 99% time in function\n",
    "        if response.status_code != 200:\n",
    "            print('this boat\\'s info is 404')\n",
    "            return None\n",
    "        \n",
    "        soup=BeautifulSoup(response.content,'html.parser')\n",
    "        title=soup.find('title')\n",
    "        first_table=soup.find('table',class_='aparams')#for IMO/MMSI,Callsign\n",
    "        describe_text = soup.find('p', class_='text2')\n",
    "\n",
    "\n",
    "        imo = mmsi = Callsign = flag = name = ship_type = length = beam = '-'\n",
    "\n",
    "\n",
    "        if title:\n",
    "            print(title.text)\n",
    "            # from title,get name,type:\n",
    "            title_date=re.match(r'(.+),\\s(.+)\\s-\\sDetails\\sand\\scurrent\\sposition\\s-\\s(.+)\\s(.+)\\s-\\sV',title.text)\n",
    "            # if title_date.group(3)=='IMO':\n",
    "            #     imo=title_date.group(4)\n",
    "            \n",
    "                        \n",
    "        #     if title_date.group(4)=='MMSI':\n",
    "        #         mmsi=title_date.group(4)\n",
    "        #         if first_table:\n",
    "        #             first_table_data=first_table.find_all('td',class_='v3')\n",
    "        #             # split_imo_mmsi=re.match(r'(.+)/(.+)',first_table_data[6].text)#make IMO and MMSI of the first table seperate\n",
    "        #             if first_table_data[9].text!='-':\n",
    "        #                 split_igth_beam=re.match(r'(.+)/(.+)m',first_table_data[9].text)#make length and beam of the first table seperate\n",
    "        #                 length=split_igth_beam.group(1)\n",
    "        #                 beam=split_igth_beam.group(2)\n",
    "\n",
    "            name=title_date.group(1)\n",
    "            ship_type=title_date.group(2)\n",
    "        if describe_text:\n",
    "            # Pattern to extract flag\n",
    "            text = describe_text.get_text(strip=True)\n",
    "            clean_text = ' '.join(text.split())  # 移除多余的空白字符\n",
    "            # print(clean_text)\n",
    "            # second_table=soup.find('table',class_='tparams')#for name and type\n",
    "            flag_pattern = r\"sailing under the flag of(.+)\\.\"\n",
    "            flag_match = re.search(flag_pattern, clean_text)\n",
    "            if flag_match:\n",
    "                flag = flag_match.group(1).strip()\n",
    "\n",
    "            # Pattern to extract IMO\n",
    "            imo_pattern = r\"IMO (\\d+)\"\n",
    "            imo_match = re.search(imo_pattern, clean_text)\n",
    "            if imo_match:\n",
    "                imo = imo_match.group(1).strip()\n",
    "\n",
    "            # Pattern to extract MMSI\n",
    "            mmsi_pattern = r\"MMSI (\\d+)\"\n",
    "            mmsi_match = re.search(mmsi_pattern, clean_text)\n",
    "            if mmsi_match:\n",
    "                mmsi = mmsi_match.group(1).strip()\n",
    "\n",
    "            # Pattern to extract type\n",
    "            type_pattern = r\"is a ([A-Za-z\\s]+) built\"\n",
    "            type_match = re.search(type_pattern, clean_text)\n",
    "            if not type_match:  # Fallback for the other type of sentence structure\n",
    "                type_pattern = r\"is a ([A-Za-z\\s]+) and currently\"\n",
    "                type_match = re.search(type_pattern, clean_text)\n",
    "            if type_match:\n",
    "                ship_type = type_match.group(1).strip()\n",
    "\n",
    "        if first_table:\n",
    "            first_table_data=first_table.find_all('td',class_='v3')\n",
    "            # split_imo_mmsi=re.match(r'(.+)/(.+)',first_table_data[6].text)#make IMO and MMSI of the first table seperate\n",
    "            # mmsi=split_imo_mmsi.group(2)\n",
    "            if first_table_data[9].text!='-':\n",
    "                split_igth_beam=re.match(r'(.+)/(.+)m',first_table_data[9].text)#make length and beam of the first table seperate\n",
    "                length=split_igth_beam.group(1)\n",
    "                beam=split_igth_beam.group(2)\n",
    "\n",
    "\n",
    "            Callsign=first_table_data[7].text\n",
    "            # flag=first_table_data[8].text\n",
    "\n",
    "        # if second_table:\n",
    "        #     second_table_data=second_table.find_all('td',class_='v3')\n",
    "        #     name=second_table_data[1].text\n",
    "        #     ship_type=second_table_data[2].text\n",
    "        #     gross_Tonnage=second_table_data[5].text\n",
    "        #     summer_Deadweight=second_table_data[6].text\n",
    "        '''\n",
    "        正则表达式：\n",
    "        一个问题，为什么不能直接匹配table.text中的数据？有待调查\n",
    "        '''\n",
    "        #from title,get name and type:\n",
    "        # title_date=re.match(r'(.+),\\s(.+)\\s-\\sD',title.text)\n",
    "\n",
    "        #used for debug:\n",
    "        # print(table_data.group)\n",
    "        # print(second_table_data[1].text)\n",
    "\n",
    "        this_ship=[imo,mmsi,Callsign,name,ship_type,flag,length,beam]\n",
    "\n",
    "        with open('test_for_vessel_finder'+str(num_csv)+'.csv',mode='a',newline='') as vfinder:\n",
    "            writer=csv.writer(vfinder)\n",
    "            writer.writerow(this_ship)\n",
    "        print(this_ship)\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        save_progress(url, page, flag_index, type_value, num_csv, scrapying_page_num, current_boat_index, progress_filename)\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "find_all 方法返回的是一个 ResultSet 对象。\n",
    "ResultSet 是 BeautifulSoup 库中用于存储所有匹配结果的容器，它的行为类似于一个 Python 列表。\n",
    "ResultSet 对象包含所有匹配的标签，每个标签都是一个 Tag 对象。\n",
    "\n",
    "ResultSet 对象中的每个元素都是一个 Tag 对象。\n",
    "Tag 对象表示一个 HTML 或 XML 标签，并且包含了该标签的属性、文本内容以及子标签。\n",
    "'''\n",
    "\n",
    "def front_page_analysis(url):\n",
    "    global page, flag_index, type_value, num_csv, scrapying_page_num, current_boat_index\n",
    "    previous_boat_info_url = None\n",
    "    print('current scraping url', url)\n",
    "    try:\n",
    "        time.sleep(random.uniform(1, 3))  # 随机延迟\n",
    "        response = requests.get(url, cookies=cookies, headers=headers)  # cost 99% time before for\n",
    "        if response.status_code != 200:\n",
    "            print('no more page')\n",
    "            return False\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        boats_info = soup.find_all('a', class_='ship-link')  # all boats' link of current page\n",
    "\n",
    "        if not boats_info:  # Check if the list is empty\n",
    "            print(\"No boats found on this page.\")\n",
    "            return False  # Indicate no boats found\n",
    "        else:\n",
    "            for i in range(current_boat_index, len(boats_info)):\n",
    "                boat_info = boats_info[i]\n",
    "                present_boat_info_url = base_url + boat_info['href']\n",
    "                if previous_boat_info_url != present_boat_info_url:\n",
    "                    print('now is', present_boat_info_url)\n",
    "                    boat_info_analysis(present_boat_info_url)\n",
    "                    previous_boat_info_url = present_boat_info_url\n",
    "                current_boat_index = i + 1\n",
    "            scrapying_page_num += 1  # record that already scraping 1 page\n",
    "            current_boat_index = 0  # Reset current_boat_index after processing all boats in the current page\n",
    "            return True  # Indicate boats found\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        save_progress(url, page, flag_index, type_value, num_csv, scrapying_page_num, current_boat_index, progress_filename)\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恢复爬取进度\n",
    "progress_data = load_progress(progress_filename)\n",
    "print(progress_data)\n",
    "if progress_data:\n",
    "    current_url = progress_data['current_url']\n",
    "    page = progress_data['page']\n",
    "    flag_index = progress_data['flag_index']\n",
    "    type_value = progress_data['type_value']\n",
    "    num_csv = progress_data['num_csv']\n",
    "    scrapying_page_num = progress_data['scrapying_page_num']\n",
    "    current_boat_index = progress_data['current_boat_index']\n",
    "    print(f\"Resuming from {current_url}\",' and number',current_boat_index)\n",
    "    print('continue write in csv', num_csv)\n",
    "else:\n",
    "    current_url = base_url + '/vessels'\n",
    "    page = 1\n",
    "    flag_index = 0\n",
    "    type_value = 0\n",
    "    num_csv = 1\n",
    "    scrapying_page_num = 0\n",
    "    current_boat_index = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "initial_page = requests.get(current_url, cookies=cookies, headers=headers)\n",
    "initial_page_soup = BeautifulSoup(initial_page.content, 'html.parser')\n",
    "flag_select = initial_page_soup.find('select', id=\"advsearch-ship-flag\")\n",
    "all_flags = flag_select.find_all('option')\n",
    "\n",
    "# 创建CSV文件并写入表头（如果文件不存在）\n",
    "\n",
    "for flag_index in range(flag_index, len(all_flags)):\n",
    "    flag = all_flags[flag_index]\n",
    "    if flag['value'] != '-':\n",
    "        flag_value = flag['value']\n",
    "        for type_value in range(type_value, 9):\n",
    "            while page <= 200:\n",
    "                if scrapying_page_num>=500:\n",
    "                    num_csv+=1\n",
    "                    with open('test_for_vessel_finder'+str(num_csv)+'.csv', mode='a', newline='') as vfinder:\n",
    "                        writer = csv.writer(vfinder)\n",
    "                        writer.writerow(['IMO','MMSI', 'Callsign', 'Vessel Name', 'Ship type', 'Flag',  'Length Overall (m)', 'Beam (m)'])\n",
    "                    scrapying_page_num=0\n",
    "                url = f'{base_url}/vessels?page={page}&type={type_value}&flag={flag_value}'\n",
    "                \n",
    "                boats_found = front_page_analysis(url)\n",
    "                if not boats_found:  # No boats found for this type and flag, skip to next type\n",
    "                    break\n",
    "\n",
    "                page += 1\n",
    "            page = 1\n",
    "        type_value = 0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
